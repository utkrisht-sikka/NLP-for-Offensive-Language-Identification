{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74b83b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (0.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: torchvision in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (4.24.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.9.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (3.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python310\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.10.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install sentence-transformers\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef78b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import linear_model\n",
    "from  sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn import svm\n",
    "import torch \n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sentence_transformers import InputExample, losses, models\n",
    "from sentence_transformers import util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c61af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"H1_train.csv\", sep=\",\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fd4443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label  id\n",
       "0  @USER She should ask a few native Americans wh...   OFF   0\n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...   OFF   1\n",
       "2  Amazon is investigating Chinese employees who ...   NOT   2\n",
       "3  @USER Someone should'veTaken\" this piece of sh...   OFF   3\n",
       "4  @USER @USER Obama wanted liberals &amp; illega...   NOT   4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e6184c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_CONEXTUAL_MODEL_TYPE = \"glove.6B.50d.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d106a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "OUT_DIM_DENSE = 256\n",
    "NUM_EPOCHS = 3 ## THIS IS FIXED DO NOT CHANGE ; changed from 2 --> 3 since MAX NUM EPOCHS = 3\n",
    "\n",
    "MAX_SEQ_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a75ced45",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDINGS_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccb4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"label\"]=df_train[\"label\"].replace(to_replace=\"OFF\",value=0)\n",
    "df_train[\"label\"]=df_train[\"label\"].replace(to_replace=\"NOT\",value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3eeaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text):\n",
    "    list_of_words = re.sub(r\"^A-Za-z\", \" \", raw_text).lower().split()\n",
    "    return list_of_words\n",
    "    \n",
    "def get_sentence_embedding(sentence, embeddings_dict):\n",
    "    # While creating sentence vector, only consider words that have a key in the glove dict\n",
    "    \n",
    "    relevant_embeddings = [embeddings_dict[word] for word in preprocess(sentence) if word in embeddings_dict.keys()]\n",
    "    if len(relevant_embeddings) > 0:\n",
    "        sentence_embedding = np.mean(relevant_embeddings, axis=0)\n",
    "    else:\n",
    "        sentence_embedding = np.zeros(GLOVE_EMBEDDINGS_DIM)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a8b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model1():\n",
    "    \"\"\"\n",
    "    return glove embeddings dict\n",
    "    \"\"\"\n",
    "    # https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\n",
    "    embeddings_dict = dict()\n",
    "    with open(NON_CONEXTUAL_MODEL_TYPE, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    \n",
    "    print(len(embeddings_dict))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf4a9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_model1(data_frame):\n",
    "    \"\"\"\n",
    "    Input a data frame and return the embedding vectors for the each sentence column using non_cont_model1,\n",
    "    Return 2 matrices each of shape (#_samples, #size_of_word_emb).\n",
    "    \"\"\"\n",
    "\n",
    "    # https://www.kaggle.com/code/adepvenugopal/nlp-text-similarity-using-glove-embedding?scriptVersionId=105939541&cellId=2\n",
    "    S1 = np.array([get_sentence_embedding(sentence, non_cont_model1) for sentence in data_frame[\"tweet\"]])\n",
    "#     S2 = np.array([get_sentence_embedding(sentence, non_cont_model1) for sentence in data_frame[\"S2\"]])\n",
    "    \n",
    "    return S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a1030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9653df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_train[\"tweet\"], df_train[\"label\"], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52c0cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8870,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e66f95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10126    1\n",
       "11593    1\n",
       "11078    1\n",
       "1963     1\n",
       "3125     1\n",
       "        ..\n",
       "11964    1\n",
       "5191     1\n",
       "5390     1\n",
       "860      1\n",
       "7270     1\n",
       "Name: label, Length: 8870, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "381010f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame({\"tweet\": X_val, \"label\": y_val})\n",
    "df_train=pd.DataFrame({\"tweet\": X_train, \"label\": y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d6a0e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(8870, 50)\n",
      "(4370, 50)\n",
      "(8870, 50) (8870,)\n",
      "(4370, 50) (4370,)\n"
     ]
    }
   ],
   "source": [
    "non_cont_model1 = get_model1()\n",
    "\n",
    "feature_1_train = get_feature_model1(df_train)\n",
    "print(feature_1_train.shape)\n",
    "\n",
    "# feature_1_dev, feature_2_dev = get_feature_model1(df_dev)\n",
    "# print(feature_1_dev.shape, feature_2_dev.shape)\n",
    "\n",
    "feature_1_test = get_feature_model1(df_test)\n",
    "print(feature_1_test.shape)\n",
    "\n",
    "X_train, Y_train = np.array(feature_1_train), np.array(df_train[\"label\"])\n",
    "# X_dev, Y_dev = np.hstack((feature_1_dev, feature_2_dev)), np.array(df_dev[\"SCORE\"]).astype(np.float32\n",
    "X_test, Y_test = feature_1_test, np.array(df_test[\"label\"])\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "# print(X_dev.shape, Y_dev.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f01f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\udit1\\appdata\\roaming\\python\\python310\\site-packages (from gensim) (5.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99e65a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\udit1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7867bd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.17      0.26      1467\n",
      "           1       0.69      0.95      0.80      2903\n",
      "\n",
      "    accuracy                           0.68      4370\n",
      "   macro avg       0.65      0.56      0.53      4370\n",
      "weighted avg       0.66      0.68      0.62      4370\n",
      "\n",
      "Logistic Regression, Accuracy Score: 0.6844393592677346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit1\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logictic_model = LogisticRegression().fit(X_train,Y_train)\n",
    "y_preds = logictic_model.predict(X_test)\n",
    "report = classification_report( Y_test, y_preds )\n",
    "print(report)\n",
    "acc=accuracy_score(Y_test,y_preds)\n",
    "print(\"Logistic Regression, Accuracy Score:\" , acc)\n",
    "# dict_models_tfidf[\"logictic_model\"]=logictic_model\n",
    "# dict_accuracy_report_tfidf[\"Logistic Regression\"]=report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "985df563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Test Set Accuracy : 67.48283752860412 %\n",
      "\n",
      "\n",
      "\n",
      "accuracy_matrix\n",
      "[[ 120 1347]\n",
      " [  74 2829]]\n",
      "\n",
      "Classification Report : \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.08      0.14      1467\n",
      "           1       0.68      0.97      0.80      2903\n",
      "\n",
      "    accuracy                           0.67      4370\n",
      "   macro avg       0.65      0.53      0.47      4370\n",
      "weighted avg       0.66      0.67      0.58      4370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_pa = PassiveAggressiveClassifier(C = 0.6, random_state = 5)\n",
    "model_pa.fit(X_train, Y_train)\n",
    "test_pred = model_pa.predict(X_test)\n",
    "score = accuracy_score(Y_test, test_pred)\n",
    "print(f\"Tuned Test Set Accuracy : {accuracy_score(Y_test, test_pred) * 100} %\\n\\n\")  \n",
    "print()\n",
    "print(\"accuracy_matrix\")\n",
    "print(confusion_matrix(Y_test,test_pred))\n",
    "print()\n",
    "report=classification_report(Y_test, test_pred)\n",
    "print(f\"Classification Report : \\n\\n{classification_report(Y_test, test_pred)}\")\n",
    "# dict_models[\"passive_aggresive\"]=model_pa\n",
    "# dict_accuracy[score]=\"passive_aggresive\"\n",
    "\n",
    "# dict_models_tfidf[\"passive_aggresive\"]=model_pa\n",
    "# dict_accuracy_report_tfidf[\"passive_aggresive\"]=report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba2ba7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96c7eee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8870, 50)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ea3c3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'udit', 'narang1243', 'udit']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"my name is udit narang1243 Udit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f91a894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_len(sentences):\n",
    "    n=0\n",
    "    for sentence in sentences:\n",
    "        list_of_words=preprocess(sentence)\n",
    "        if n<len(list_of_words):\n",
    "            n=len(list_of_words)\n",
    "    return n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a5ada97",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_len=find_max_len(df_train[\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a535fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e947c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.zeros((len(df_train), 50*(max_train_len+10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a37ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_embedding=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c86ceea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, embeddings_dict):\n",
    "    # While creating sentence vector, only consider words that have a key in the glove dict\n",
    "    sentence_embedding=np.zeros(len_embedding)\n",
    "    relevant_embeddings = [embeddings_dict[word] for word in preprocess(sentence) if word in embeddings_dict.keys()]\n",
    "    if len(relevant_embeddings) > 0:\n",
    "        for i in range(len(relevant_embeddings)):\n",
    "            for j in range(50):\n",
    "                sentence_embedding[i*50+j]=relevant_embeddings[i][j]\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aafc32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f489e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"tweet\"], df_train[\"label\"], test_size=0.245, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b3a62b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6696,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46c37b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.DataFrame({\"tweet\": X_train, \"label\": y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b722dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6696, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bb4a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"small_train_set.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
